{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for the development of the Sentiment Analysis script that was used in the Trading Systems Development IQP. The indicator attempts to measure the sentiment of news headlines, which will be used to predict upcomming movements in the markets. \n",
    "\n",
    "The headlines are scraped using Selenium from the Investing.com website. The headlines are then preprocessed and their sentiment analyzed using the lexicon approach described by Loughran and McDonald [1]. The lexicon approach entails the use of two lists of words, one negative and one positive, counting the number of each type of word as well as the frequency of each individual word. This method yields a score for each headline, which is then aggregated on a daily basis to determine the score of the day.\n",
    "\n",
    "[1] \"When is a liability not a liability? Textual Analysis, Dictionaries, and 10-Ks\" https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1331573 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentiment score is calculated using the following formula:\n",
    "\n",
    "$$\n",
    "f(x) = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            \\frac{(1+log(tf_{i,j}))}{(1+log(a))}*log(\\frac{N}{df_{i,j}}) & tf_{i,j} \\geq 1\\\\\n",
    "            0 & otherwise\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "$$\n",
    "\n",
    "Where the following variables are used:\n",
    "\n",
    "$$\n",
    "\\text{N, the total number of headlines in the sample.}\n",
    "\\\\\n",
    "\\text{a, the average word count in the sample.}\n",
    "\\\\\n",
    "\\text{ $tf_{i,j}$, the raw count of the $i^{th}$ word in the $j^{th}$ document.}\n",
    "\\\\\n",
    "\\text{$df_i$, the number of headlines containing at least one occurrence of the $i^{th}$ word.}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines preload the data, import necessary libraries and create user defined functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "#import collocation_analysis as ca\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "#from investing_scrape import scrape_headlines\n",
    "import numpy as np\n",
    "import re\n",
    "from decimal import Decimal\n",
    "import json\n",
    "import time\n",
    "import unicodedata\n",
    "from pandas import DataFrame\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "\n",
    "#region FUNCTION DEFINITIONS\n",
    "def clean_Text(text):\n",
    "    \"\"\"\n",
    "    removes punctuation, stopwords and returns lowercase text in a list of single words\n",
    "    \"\"\"    \n",
    "    print(text)\n",
    "    # If the string happens to be NaN, the regexp module will throw an error. By checking for the length of the \n",
    "    # string I can catch whether it is a string or not for a TypeError will be thrown if it is not a string. \n",
    "    try:\n",
    "        t = len(text)\n",
    "        #Tokenize the text\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        text = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", text)\n",
    "        text = text.lower()\n",
    "        text = tokenizer.tokenize(text)\n",
    "\n",
    "        #Clean the text\n",
    "        clean = [word for word in text if word not in stopwords.words('english')]\n",
    "\n",
    "        return clean\n",
    "    \n",
    "    except TypeError:\n",
    "        return \"null\"\n",
    "\n",
    "# Load the list of positive words\n",
    "def loadPositive():\n",
    "    #filepath = os.path.dirname(os.path.abspath(__file__))\n",
    "    #C:\\\\Users\\\\afn\\\\Desktop\\\\Intern_Projects\\\\\n",
    "    #C:\\\\Users\\\\Alan Fernandez\\\\IQPython\\\\fxnews\\\\fxnews\\\\csv_files\\\\\n",
    "    with open('C:\\\\Users\\\\Alan Fernandez\\\\IQPython\\\\fxnews\\\\fxnews\\\\csv_files\\\\LMC_Positive.csv', 'r') as f:\n",
    "        pos_lines = f.readlines()\n",
    "        pos_lex_lists = [word.strip() for word in pos_lines]\n",
    "        return pos_lex_lists\n",
    "\n",
    "\n",
    "# Load the list of negative words\n",
    "def loadNegative():\n",
    "    #filepath = os.path.dirname(os.path.abspath(__file__))\n",
    "    #C:\\\\Users\\\\afn\\\\Desktop\\\\Intern_Projects\\\\\n",
    "    #C:\\\\Users\\\\Alan Fernandez\\\\IQPython\\\\fxnews\\\\fxnews\\\\csv_files\\\\\n",
    "    with open('C:\\\\Users\\\\Alan Fernandez\\\\IQPython\\\\fxnews\\\\fxnews\\\\csv_files\\\\LMC_Negative.csv', 'r') as f:\n",
    "        neg_lines = f.readlines()\n",
    "        neg_lex_lists = [word.strip() for word in neg_lines]\n",
    "        return neg_lex_lists\n",
    "\n",
    "\n",
    "def countNegative(sentence, neg_lex_list, total_headline_count, doc_average_word_count):\n",
    "    # Sentence is a sentence broken up into a list of words\n",
    "    # Add a row to the df_frequency dataframe that represents the weight assigned to that word\n",
    "    # Use the Loughran & McDonald formula to create the weight:\n",
    "    # Wij = ((1+log(tfij))/(1+log(a)))*log(N/dfi)\n",
    "    # Where N represents the total number of headlines in the sample\n",
    "    # dfi the number of headlines containing at least one occurrence of the ith word\n",
    "    # tfij the raw count of the ith word in the jth document\n",
    "    # a the average word count in the document\n",
    "    N = total_headline_count\n",
    "    headline_score = 0\n",
    "    a = doc_average_word_count.item()\n",
    "    for word in sentence:\n",
    "        if word.upper() in neg_lex_list:\n",
    "            # Find the index of the word then obtain the corresponding frequency and dfi\n",
    "            wIndex = list(np.where(df_frequency[\"word\"] == word.lower())[0])[0]\n",
    "            dfi = df_frequency[\"dfi\"][wIndex]\n",
    "            # Determine the frequency of this word in this particular headline\n",
    "            local_freq = sum(1 for _ in re.finditer(r'\\b%s\\b' % re.escape(word), ' '.join(sentence)))\n",
    "            # Calculate the weight assigned to this word\n",
    "            if local_freq == 0 or a == 0 or N/dfi == 0:\n",
    "                print('*******************************')\n",
    "                print(local_freq)\n",
    "                print('*******************************')\n",
    "                \n",
    "            weight = ((1+np.log(local_freq))/(1+np.log(a)))*np.log(N/dfi)\n",
    "            # Add this word score to the headline's score\n",
    "            if weight != 0:\n",
    "                headline_score += weight.item()\n",
    "            else:\n",
    "                headlines_score += 1\n",
    "\n",
    "    return headline_score\n",
    "\n",
    "\n",
    "def countPositive(sentence, pos_lex_list, total_headline_count, doc_average_word_count):\n",
    "    # Sentence is a sentence broken up into a list of words\n",
    "    # Add a row to the df_frequency dataframe that represents the weight assigned to that word\n",
    "    # Use the Loughran & McDonald formula to create the weight:\n",
    "    # Wij = ((1+log(tfij))/(1+log(a)))*log(N/dfi)\n",
    "    # Where N represents the total number of headlines in the sample\n",
    "    # dfi the number of headlines containing at least one occurrence of the ith word\n",
    "    # tfij the raw count of the ith word in the jth document\n",
    "    # a the average word count in the document\n",
    "    N = total_headline_count\n",
    "    headline_score = 0\n",
    "    a = doc_average_word_count.item()\n",
    "    for word in sentence:\n",
    "        if word.upper() in pos_lex_list:\n",
    "            # Find the index of the word then obtain the corresponding frequency and dfi\n",
    "            wIndex = list(np.where(df_frequency[\"word\"] == word.lower())[0])[0]\n",
    "            dfi = df_frequency[\"dfi\"][wIndex]\n",
    "            # Determine the frequency of this word in this particular headline\n",
    "            local_freq = sum(1 for _ in re.finditer(r'\\b%s\\b' % re.escape(word), ' '.join(sentence)))\n",
    "            # Calculate the weight assigned to this word\n",
    "            if local_freq == 0 or a == 0 or N/dfi == 0:\n",
    "                print('*******************************')\n",
    "                print(local_freq)\n",
    "                print('*******************************')\n",
    "                \n",
    "            weight = ((1+np.log(local_freq))/(1+np.log(a)))*np.log(N/dfi)\n",
    "            # Add this word score to the headline's score\n",
    "            if weight != 0:\n",
    "                headline_score += weight.item()\n",
    "            else:\n",
    "                headlines_score += 1\n",
    "\n",
    "    return headline_score\n",
    "\n",
    "\n",
    "def calculateSentiment(clean_headline, positive_list, negative_list, awc):\n",
    "    # print(\"CALCULATING SENTIMENT\")\n",
    "    lexicon_negative = countNegative(clean_headline, negative_list, len(df_headlines), awc)\n",
    "    lexicon_positive = countPositive(clean_headline, positive_list, len(df_headlines), awc)\n",
    "    return lexicon_positive-lexicon_negative\n",
    "\n",
    "# Using webdriver to launch the website and allow the javascript to load. Once the JS loads,\n",
    "# it is possible to extract the source code -> innerHTML\n",
    "# BeautifulSoup processes the innerHTML, which is then parsed for the desired sections.\n",
    "# Title, date and url are all stored in a dictionary that has the title as the key.\n",
    "# The dictionary is stored in a json file for further usage.\n",
    "def scrape_headlines(starting_page, ending_page, section):\n",
    "    browser = webdriver.Chrome()\n",
    "    url_upto_item = \"https://www.investing.com/currencies/\" + section + '/'\n",
    "    page_number = starting_page\n",
    "\n",
    "    # Create a dictionary to store these values. Each title is the key to an entry.\n",
    "    news = {}\n",
    "    article_total = 0\n",
    "\n",
    "    # try:\n",
    "    #     news[article_total][\"source\"] = element.span.get_text()[3:]\n",
    "    # except AttributeError:\n",
    "\n",
    "    # Loop through all the pages\n",
    "    while page_number <= ending_page:\n",
    "\n",
    "        # Join parts of url\n",
    "        url = url_upto_item + str(page_number)\n",
    "        # Navigate browser to url\n",
    "        browser.get(url)\n",
    "\n",
    "        # Wait 10 seconds for the js to finish loading the webpage\n",
    "        time.sleep(2)\n",
    "        # Get innerHTML from webpage\n",
    "        inner_html = browser.execute_script(\"return document.body.innerHTML\")\n",
    "        # Parse the innerHTML\n",
    "        soup = BeautifulSoup(inner_html, 'html.parser')\n",
    "\n",
    "        # Find all the classes that contain the desired information.\n",
    "        # Title, url and date are all contained within the 'fxs_floatingMedia..' class in div objects.\n",
    "        items = soup.find_all(\"article\", class_=\"articleItem\")\n",
    "\n",
    "        article_counter = 1\n",
    "        # Clean the gathered text before putting it in the dictionary\n",
    "        for element in items:\n",
    "            # Last two articles are not real articles rather some extra stuff on the webpage\n",
    "            if article_counter < len(items)-2:\n",
    "                news[article_total] = {}\n",
    "                news[article_total][\"title\"] = element.div.a[\"title\"]\n",
    "                print(element.div.a[\"title\"])\n",
    "                news[article_total][\"date\"] = element.findAll(\"span\", {\"class\": \"date\"})[0].get_text()[3:]\n",
    "\n",
    "                # The href standards changed at some point. They are stored in two ways. Here I account for that difference\n",
    "                if \"https://invst.ly\" in element.div.a[\"href\"]:\n",
    "                    news[article_total][\"url\"] = element.div.a[\"href\"]\n",
    "                else:\n",
    "                    news[article_total][\"url\"] = \"https://investing.com\" + element.div.a[\"href\"]\n",
    "\n",
    "                try:\n",
    "                    news[article_total][\"source\"] = element.span.span.get_text()[3:]\n",
    "                except AttributeError:\n",
    "                    news[article_total][\"source\"] = element.span.get_text()[3:]\n",
    "\n",
    "            article_counter += 1\n",
    "            article_total += 1\n",
    "\n",
    "        # Increment page number by 1 to navigate to the next page\n",
    "        page_number += 1\n",
    "\n",
    "    # ----------------------- DATAFRAME -----------------------\n",
    "\n",
    "    # create the dataframe from the dictionary\n",
    "    df = pd.DataFrame.from_dict(news)\n",
    "\n",
    "    # transpose the columns/rows\n",
    "    df = df.T\n",
    "\n",
    "    # Sort by date\n",
    "    # df = df.sort_values(by=['date'])\n",
    "\n",
    "    # ----------------------- SAVING -----------------------\n",
    "    filename = 'headlines_InvestingCom_' + str(starting_page) + '_to_' + str(ending_page) + '.csv'\n",
    "    # Save the dataframe to a csv file\n",
    "    df.to_csv(filename, encoding='utf-8')\n",
    "\n",
    "    print(\"***********************\")\n",
    "    print(\"Headline Scraping Done.\")\n",
    "    print(\"***********************\")\n",
    "\n",
    "    browser.close()\n",
    "\n",
    "    return df\n",
    "\n",
    "def wordFrequency(df, content_column):\n",
    "    # Create a dictionary where to store the words and counts\n",
    "    frequency = {}\n",
    "\n",
    "    # Iterate through each row and count the frequency of words\n",
    "    for index, row in df.iterrows():\n",
    "        # First clean the text\n",
    "        text_string = clean_Text(row[content_column])\n",
    "\n",
    "        # Count word frequency\n",
    "        for word in text_string:\n",
    "            if word in frequency:\n",
    "                frequency[word] += 1\n",
    "            else:\n",
    "                frequency[word] = 1\n",
    "\n",
    "        print(frequency)\n",
    "    # Create the list with the counts\n",
    "    frequency_list = frequency.keys()\n",
    "\n",
    "    # Print words and counts\n",
    "    for words in frequency_list:\n",
    "        print(words, frequency[words])\n",
    "\n",
    "    word_df = pd.DataFrame(list(frequency.items()))\n",
    "    word_df = word_df.T\n",
    "\n",
    "    # Save the frequency dictionary to a csv file for further analysis\n",
    "    with open('word_frequency.csv', 'w') as csv_file:\n",
    "        word_df.to_csv(csv_file, header=False)\n",
    "    csv_file.close()\n",
    "\n",
    "    return word_df\n",
    "\n",
    "# Using webdriver to launch the website and allow the javascript to load. Once the JS loads,\n",
    "# it is possible to extract the source code -> innerHTML\n",
    "# BeautifulSoup processes the innerHTML, which is then parsed for the desired sections.\n",
    "# Title, date and url are all stored in a dictionary that has the title as the key.\n",
    "# The dictionary is stored in a json file for further usage.\n",
    "def scrape_headlines(starting_page, ending_page, section):\n",
    "    browser = webdriver.Chrome()\n",
    "    url_upto_item = \"https://www.investing.com/currencies/\" + section + '/'\n",
    "    page_number = starting_page\n",
    "\n",
    "    # Create a dictionary to store these values. Each title is the key to an entry.\n",
    "    news = {}\n",
    "    article_total = 0\n",
    "\n",
    "    # try:\n",
    "    #     news[article_total][\"source\"] = element.span.get_text()[3:]\n",
    "    # except AttributeError:\n",
    "\n",
    "    # Loop through all the pages\n",
    "    while page_number <= ending_page:\n",
    "\n",
    "        # Join parts of url\n",
    "        url = url_upto_item + str(page_number)\n",
    "        # Navigate browser to url\n",
    "        browser.get(url)\n",
    "\n",
    "        # Wait 10 seconds for the js to finish loading the webpage\n",
    "        time.sleep(2)\n",
    "        # Get innerHTML from webpage\n",
    "        inner_html = browser.execute_script(\"return document.body.innerHTML\")\n",
    "        # Parse the innerHTML\n",
    "        soup = BeautifulSoup(inner_html, 'html.parser')\n",
    "\n",
    "        # Find all the classes that contain the desired information.\n",
    "        # Title, url and date are all contained within the 'fxs_floatingMedia..' class in div objects.\n",
    "        items = soup.find_all(\"article\", class_=\"articleItem\")\n",
    "\n",
    "        article_counter = 1\n",
    "        # Clean the gathered text before putting it in the dictionary\n",
    "        for element in items:\n",
    "            # Last two articles are not real articles rather some extra stuff on the webpage\n",
    "            if article_counter < len(items)-2:\n",
    "                news[article_total] = {}\n",
    "                news[article_total][\"title\"] = element.div.a[\"title\"]\n",
    "                print(element.div.a[\"title\"])\n",
    "                news[article_total][\"date\"] = element.findAll(\"span\", {\"class\": \"date\"})[0].get_text()[3:]\n",
    "\n",
    "                # The href standards changed at some point. They are stored in two ways. Here I account for that difference\n",
    "                if \"https://invst.ly\" in element.div.a[\"href\"]:\n",
    "                    news[article_total][\"url\"] = element.div.a[\"href\"]\n",
    "                else:\n",
    "                    news[article_total][\"url\"] = \"https://investing.com\" + element.div.a[\"href\"]\n",
    "\n",
    "                try:\n",
    "                    news[article_total][\"source\"] = element.span.span.get_text()[3:]\n",
    "                except AttributeError:\n",
    "                    news[article_total][\"source\"] = element.span.get_text()[3:]\n",
    "\n",
    "            article_counter += 1\n",
    "            article_total += 1\n",
    "\n",
    "        # Increment page number by 1 to navigate to the next page\n",
    "        page_number += 1\n",
    "\n",
    "    # ----------------------- DATAFRAME -----------------------\n",
    "\n",
    "    # create the dataframe from the dictionary\n",
    "    df = pd.DataFrame.from_dict(news)\n",
    "\n",
    "    # transpose the columns/rows\n",
    "    df = df.T\n",
    "\n",
    "    # Sort by date\n",
    "    # df = df.sort_values(by=['date'])\n",
    "\n",
    "    # ----------------------- SAVING -----------------------\n",
    "    filename = 'headlines_InvestingCom_' + str(starting_page) + '_to_' + str(ending_page) + '.csv'\n",
    "    # Save the dataframe to a csv file\n",
    "    df.to_csv(filename, encoding='utf-8')\n",
    "\n",
    "    print(\"***********************\")\n",
    "    print(\"Headline Scraping Done.\")\n",
    "    print(\"***********************\")\n",
    "\n",
    "    browser.close()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# This function iterates through the dataframe and removes advert entries as well as corrects dates that are improperly formatted\n",
    "# as a number of hours ago rather than a date.\n",
    "def clean_dataframe(df):\n",
    "    #global df_headlines\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    df_headlines_clean = df.copy()\n",
    "    df_headlines_cp = df.copy()\n",
    "    index_remove = []\n",
    "    \n",
    "    # Inneficiently remove ads and clean up the dataframe.\n",
    "    # Iterating multiple times because the index will not correspond after something gets removed.\n",
    "    df_headlines_cp = df_headlines_clean.copy()\n",
    "    for index, row in df_headlines_cp.iterrows():\n",
    "        # Replace \"X hours ago\" entries for today's date\n",
    "        if 'hour' in row['date'] or 'hours' in row['date']:\n",
    "            df_headlines_clean.iloc[index]['date'] = now.strftime('%b %d, %Y')\n",
    "    \n",
    "    df_headlines_cp = df_headlines_clean.copy()\n",
    "    for index, row in df_headlines_cp.iterrows():\n",
    "        if 'https://investing.com/education/' in row['url']:\n",
    "            df_headlines_clean = df_headlines_clean.drop([index])    \n",
    "    \n",
    "    df_headlines_cp = df_headlines_clean.copy()\n",
    "    for index, row in df_headlines_cp.iterrows():\n",
    "        if 'EDT' in row['date']:\n",
    "            df_headlines_clean = df_headlines_clean.drop([index]) \n",
    "            \n",
    "    return df_headlines_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_headline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape the Investing.com website and obtain a list of headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_headlines = scrape_headlines(1, 943, 'usd-chf-news')\n",
    "clean_dataframe()\n",
    "df_headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create/import the dataframes containing the data to analyze. In this case the headlines have been downloaded previously, so they are imported from a csv file. The date column is converted to a datetime object for easy manipulation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_prices = pd.read_csv(\"C:\\\\Users\\\\Alan Fernandez\\\\IQPython\\\\fxnews\\\\fxnews\\\\csv_files\\\\USD_CHF_Sent_Prices.csv\")\n",
    "#df_aaii = pd.read_csv(\"C:\\\\Users\\\\Alan Fernandez\\\\IQPython\\\\fxnews\\\\fxnews\\\\csv_files\\\\aaii_sentiment.csv\")\n",
    "df_prices = pd.read_csv(\"C:\\\\Users\\\\Alan Fernandez\\\\IQPython\\\\fxnews\\\\fxnews\\\\csv_files\\\\USD_CHF_Sent_Prices.csv\")\n",
    "df_aaii = pd.read_csv(\"C:\\\\Users\\\\Alan Fernandez\\\\IQPython\\\\fxnews\\\\fxnews\\\\csv_files\\\\aaii_sentiment.csv\")\n",
    "df_headlines = pd.read_csv(\"C:\\\\Users\\\\Alan Fernandez\\\\Documents\\\\GitHub\\\\Investing-IQP-18\\\\Jupyter_MLX\\\\headlines_InvestingCom_Stock_1_to_1000.csv\")\n",
    "df_prices['date'] = pd.to_datetime(df_prices['date'])\n",
    "df_aaii['date'] = pd.to_datetime(df_aaii['date'])\n",
    "print(df_headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create lists to store calculations. These lists will eventually become the columns in the final dataframe. The positive and negative lexicon are loaded as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_lex_list = loadPositive()\n",
    "neg_lex_list = loadNegative()\n",
    "# Indicator lists\n",
    "aaii_sent_list = []\n",
    "lexicon_sent_list = []\n",
    "market_sent_list = []\n",
    "# A list of ngrams that have more weight in the analysis.\n",
    "# ngram_list = []\n",
    "ngram_match_list = []\n",
    "# Where the final aggregate sentiment is stored\n",
    "final_sent_list = []\n",
    "final_sent_list_word = []\n",
    "new_range  = [i * i for i in range(5) if i % 2 == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine word frequency in the entire database of headlines. Frequency will be used as a weight to balance \n",
    "terms with different levels of popularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frequency = wordFrequency(df_headlines, \"title\")\n",
    "\n",
    "N = len(df_headlines)\n",
    "average_word_count = df_frequency[1:2].sum(axis=1)/N\n",
    "word_occurrence = [0 for x in range(df_frequency.count(axis='columns')[1])]\n",
    "headlines_list = df_headlines['title'].tolist()\n",
    "word_list = df_frequency[0:1].values.tolist()[0]\n",
    "\n",
    "for word_index, word in enumerate(word_list):\n",
    "    for headline in headlines_list:\n",
    "        #print(word)\n",
    "        hd = RegexpTokenizer(r'\\w+').tokenize(headline)\n",
    "        hd_upper = [word.upper() for word in hd]\n",
    "        #print(hd_upper)\n",
    "        if word.upper() in hd_upper:\n",
    "            #print(word.upper())\n",
    "            word_occurrence[word_index] += 1\n",
    "\n",
    "# Add the number of occurrences to the data frame\n",
    "df_frequency = df_frequency.append([word_occurrence], ignore_index=True)\n",
    "df_frequency = df_frequency.T\n",
    "df_frequency.columns = [\"word\", \"frequency\", \"dfi\"]\n",
    "\n",
    "# Calculate the weight\n",
    "word_weight = [0 for x in range(df_frequency.count(axis='columns')[1])]\n",
    "\n",
    "print(df_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the sentiment of the headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "lexicon_sent_list = []\n",
    "for index, row in df_headlines.iterrows():\n",
    "    # First clean and tokenize the text\n",
    "    # print(\"INDEX: \" + str(index))\n",
    "    word_list = clean_Text(row[\"title\"])\n",
    "    # If one of the lists does not contain a certain date, an error will be thrown upon trying to access that date\n",
    "    # Catch that error and skip that date\n",
    "    try:\n",
    "        # row_index = df_prices.loc[df_prices['date'] == df_headlines.iloc[index]['date']]\n",
    "        # LEXICON SENTIMENT OF THE TEXT\n",
    "        # Set the sentiment to 1, -1 or 0 (Pos, Neg, Neutral)\n",
    "        # Set a flag to identify which list threw the error and to react accordingly\n",
    "        flag = 0\n",
    "        s = calculateSentiment(word_list, pos_lex_list, neg_lex_list, average_word_count)\n",
    "\n",
    "        lexicon_sent_list += [s]\n",
    "\n",
    "        #\n",
    "        # # NGRAM SENTIMENT OF THE TEXT\n",
    "        # ngram_sent_temp = checkNgram(word_list, pos_ngram_list, neg_ngram_list)\n",
    "        # if (ngram_sent_temp[0] > ngram_sent_temp[1]):\n",
    "        #     ngram_sent_list += [-1]\n",
    "        # elif (ngram_sent_temp[1] > ngram_sent_temp[0]):\n",
    "        #     ngram_sent_list += [1]\n",
    "        # else:\n",
    "        #     ngram_sent_list += [0]\n",
    "        # # Store the matching ngrams\n",
    "        # ngram_match_list += [ngram_sent_temp[2]]\n",
    "        #\n",
    "        # # MARKET SENTIMENT OF THE TEXT\n",
    "        # flag = 1\n",
    "        # market_sent_list += [1 if row_index['close'].item() - row_index['open'].item() > 0 else -1]\n",
    "\n",
    "\n",
    "    except ValueError:\n",
    "        # If the lexicon sentiment throws an error\n",
    "        if flag == 0:\n",
    "            market_sent_list += [0]\n",
    "            lexicon_sent_list += [0]\n",
    "        # If the market sentiment throws an error\n",
    "        else:\n",
    "            market_sent_list += [0]\n",
    "            lexicon_sent_list[index] = 0\n",
    "        # skipcount += 1\n",
    "        # print(\"Skip: \" + str(skipcount))\n",
    "        continue\n",
    "#endregion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now combine the sentiment results with the corresponding headlines. Since each element in the sentiment list is the sentiment of the corresponding headline (index wise), this should be as simple as appending both lists to a DF. This new DF should be a copy of df_headlines with the sentiment column appended. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_headlines_sent = df_headlines.copy()\n",
    "df_headlines_sent['sent'] = lexicon_sent_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with the DF ready with both headlines and sentiment, aggregate those sentiment scores into a final DF that includes one entry per date with the aggreate sentiment for that day. This will be the final output of the sentiment analyzer and will be imported into TradeStation. The date column should be date objects and the sentiment column should be titled 'Close' to match the format required by TS for third party data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "\n",
    "sentiment_dict = {}\n",
    "for index, row in df_headlines_sent.iterrows():\n",
    "    if index > 5:\n",
    "        # If the date already exists, then just add to the sentiment score.\n",
    "        if row['date'] not in sentiment_dict:\n",
    "            sentiment_dict[row['date']] = row['sent']\n",
    "        # If the date doesn't exist, then append it to the end of the dictionary.\n",
    "        else:\n",
    "            sentiment_dict[row['date']] += row['sent']\n",
    "\n",
    "sentiment_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.Series(sentiment_dict).to_frame()\n",
    "df_final = df_final.reset_index()\n",
    "df_final.columns = ['date', 'sent']\n",
    "df_final['date'] = pd.to_datetime(df_final.date)\n",
    "df_final.sort_values(by='date', inplace=True)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Headline and Lexicon Visualization \n",
    "Create a pie chart to visualize the percentage of headlines in each category. Also, visualize the entire lexicon by category. This comparison should help in determining whether the sentiment calculation is heavily biased towards one end of the spectrum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentage of headlines that are positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for index, row in df_final.iterrows():\n",
    "    if row['sent'] > 0:\n",
    "        a += 1\n",
    "a = a/len(df_final)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentage of headlines that are negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 0\n",
    "for index, row in df_final.iterrows():\n",
    "    if row['sent'] < 0:\n",
    "        b += 1\n",
    "b = b/len(df_final)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentage of headlines that are neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for index, row in df_final.iterrows():\n",
    "    if row['sent'] == 0:\n",
    "        c += 1\n",
    "c = c/len(df_final)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a piechart\n",
    "import matplotlib.pyplot as plt\n",
    "labels = 'Positive', 'Negative', 'Neutral'\n",
    "sizes = [a, b, c]\n",
    "colors = ['green', 'red', 'yellow']\n",
    "explode = (0.2, 0, 0)\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "plt.axis('equal')\n",
    "plt.title('Percentage of Headlines by Sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a piechart\n",
    "pos = len(pos_lex_list)/(len(neg_lex_list)+len(pos_lex_list))\n",
    "neg = len(neg_lex_list)/(len(neg_lex_list)+len(pos_lex_list))\n",
    "labels = 'Positive Words', 'Negative Words'\n",
    "sizes = [pos, neg]\n",
    "colors = ['green', 'red']\n",
    "explode = (0.2, 0,)\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "plt.axis('equal')\n",
    "plt.title('Percentage of Words in the Lexicon in Each Sentiment Category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis With NGRAM\n",
    "\n",
    "In this second sentiment analysis, NGRAMs are identified and added to the lexicon. With the ngrams it will be posible to identify and to correct for the negation of sentiment words.\n",
    "This requires a list of negation words. The script should iterate through the headline dataframe, find each instance of a negation word, check if it is followed by a sentiment word and if it is, then add the ngram to a list.\n",
    "When the sentiment calculation is performed, the headline will be checked for each ngram to correct for negation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new functions necessary for NGRAM analysis and import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import names\n",
    "import os\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from os import path\n",
    "from nltk.collocations import *\n",
    "\n",
    "def csvToList(df):\n",
    "    # Huge list containing ALL of the headlines\n",
    "    word_list = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        # First clean and tokenize the text\n",
    "        word_list += clean_Text(row[\"text\"])\n",
    "        word_sentence = ' '.join(word_list)\n",
    "    return word_list\n",
    "\n",
    "def prepareSources():\n",
    "    # Create a dictionary with three values: ((day), (all_headlines_for_day), (bar color), (empty_space_for_collocations))\n",
    "    # Import the headlines and create the list ^^\n",
    "    fgreen_db = pd.read_csv('C:\\\\Users\\\\Alan Fernandez\\\\IQPython\\\\fxnews\\\\fxnews\\\\csv_files\\\\headline_daycolor_green.csv', encoding='iso-8859-1')\n",
    "    fred_db = pd.read_csv('C:\\\\Users\\\\Alan Fernandez\\\\IQPython\\\\fxnews\\\\fxnews\\\\csv_files\\\\headline_daycolor_red.csv', encoding='iso-8859-1')\n",
    "\n",
    "    # Create a list with all the headlines\n",
    "    red_list = csvToList(fred_db)\n",
    "    green_list = csvToList(fgreen_db)\n",
    "    return [red_list, green_list]\n",
    "\n",
    "\n",
    "# Returns a list containing both lists of days\n",
    "def getBigrams(word_list, freq_filter, best_filter):\n",
    "    # Bigram Analysis\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    # Get collocations for the green day\n",
    "    bigram_finder = BigramCollocationFinder.from_words(word_list)\n",
    "    # Ignore those occuring less than \"freq_filter\" times\n",
    "    bigram_finder.apply_freq_filter(freq_filter)\n",
    "    # Get the best \"best_filter\" bigrams\n",
    "    bigram_list = bigram_finder.nbest(bigram_measures.pmi, best_filter) # doctest: +NORMALIZE_WHITESPACE\n",
    "    return bigram_list\n",
    "\n",
    "def getTrigrams(word_list, freq_filter, best_filter):\n",
    "    # Trigram Analysis\n",
    "    trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "    # Get collocations \n",
    "    trigram_finder = TrigramCollocationFinder.from_words(word_list)\n",
    "    # Ignore those occuring less than \"freq_filter\" times\n",
    "    trigram_finder.apply_freq_filter(3)\n",
    "    # Get the best best_filter trigrams\n",
    "    trigram_list = trigram_finder.nbest(trigram_measures.pmi, 30) # doctest: +NORMALIZE_WHITESPACE\n",
    "    return trigram_list\n",
    "\n",
    "def generateNgrams(pos_text_list, neg_text_list, minOcurrence, topN):\n",
    "    clean_green_list = listCleanUp(pos_text_list)\n",
    "    green_list = getBigrams(clean_green_list, minOcurrence, topN)\n",
    "    clean_red_list = listCleanUp(neg_text_list)\n",
    "    red_list = getBigrams(clean_red_list, minOcurrence, topN)\n",
    "\n",
    "    return [green_list, red_list]\n",
    "\n",
    "\n",
    "# print(\"preparing sources\")\n",
    "# lists = prepareSources()\n",
    "# print(\"generating green list\")\n",
    "# green_list = getGreenBigrams(lists[1], 5, 20)\n",
    "# print(green_list)\n",
    "# print(\"generating red list\")\n",
    "# red_list = getRedBigrams(lists[0], 5, 20)\n",
    "# print(red_list)\n",
    "# print(\"Done\")\n",
    "\n",
    "def listCleanUp(dirty_list):\n",
    "    clean_list = []\n",
    "    for item in dirty_list:\n",
    "        # First clean and tokenize the text\n",
    "        clean_list += clean_Text(item)\n",
    "    return clean_list\n",
    "\n",
    "def is_in_lexicon(word):\n",
    "    list_of_lex = [neg_lex_list, pos_lex_list]\n",
    "    for index, lst in enumerate(list_of_lex):\n",
    "        if word.upper() in lst:\n",
    "            if index == 0:\n",
    "                print(\"Neg_Lex \")\n",
    "            else:\n",
    "                print(\"Pos_Lex\")\n",
    "            print(\"True\")\n",
    "        else:\n",
    "            if index == 0:\n",
    "                print(\"Neg_Lex \")\n",
    "            else:\n",
    "                print(\"Pos_Lex\")\n",
    "            print(\"False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_title_list = df_headlines_sent[df_headlines_sent['sent'] > 0]['title'].tolist()\n",
    "neg_title_list = df_headlines_sent[df_headlines_sent['sent'] < 0]['title'].tolist()\n",
    "\n",
    "ngram_lists = generateNgrams(pos_title_list, neg_title_list, 5, 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The addition of ngrams to the lexicon analysis is sometimes considered unnecessary because one of the words in the ngram is typically already included in the lexicon, so the ngram does not bring any additional value. It is possible to account for negation using ngrams although simply searching for a negation word before a sentiment word achieves the same purpose. There are, however, certain ngrams that should contribute to the analysis such as 'trade war' which is always negative in a financial context.\n",
    "\n",
    "In the cells below the entire list of headlines is searched for ngrams that could potentially bring some value to the analysis. A comparison of how many words in the list of ngrams are also found in the lexicon is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generation of ngrams from the entire set of headlines without discrimination for their \n",
    "# sentiment score. \n",
    "minOccurence = 5\n",
    "topN = 100\n",
    "ngram_list_1 = getBigrams(listCleanUp(headlines_list), minOccurence, topN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a word is in either of the lexicon.\n",
    "count = [0,0] # where index 0 is negative and index 1 is positive count\n",
    "matches = []\n",
    "list_of_lex = [neg_lex_list, pos_lex_list]\n",
    "\n",
    "for tupl in ngram_list_1:\n",
    "    for word in tupl:\n",
    "        for index, lst in enumerate(list_of_lex):\n",
    "            if word.upper() in lst and word not in matches:\n",
    "                matches += [word]\n",
    "                print(word)\n",
    "                count[index] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, re-define the countPositive and countNegative functions to include negation detection as well as ngram detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countPositive_extra(sentence, pos_lex_list, total_headline_count, doc_average_word_count):\n",
    "    # Sentence is a sentence broken up into a list of words\n",
    "    # Add a row to the df_frequency dataframe that represents the weight assigned to that word\n",
    "    # Use the Loughran & McDonald formula to create the weight:\n",
    "    # Wij = ((1+log(tfij))/(1+log(a)))*log(N/dfi)\n",
    "    # Where N represents the total number of headlines in the sample\n",
    "    # dfi the number of headlines containing at least one occurrence of the ith word\n",
    "    # tfij the raw count of the ith word in the jth document\n",
    "    # a the average word count in the document\n",
    "    N = total_headline_count\n",
    "    headline_score = 0\n",
    "    a = doc_average_word_count\n",
    "    negation_word_list = ['not']\n",
    "    negated = 1\n",
    "    stop_list = [word for word in stopwords.words('english') if word not in ['not']]\n",
    "    tokenized_sentence = [word for word in sentence if word not in stop_list]\n",
    "    # ****** Word search ******\n",
    "    for index, word in enumerate(tokenized_sentence):\n",
    "        if word.upper() in pos_lex_list:\n",
    "            # Determine if this word is preceded by a negation word\n",
    "            if index > 0:\n",
    "                if tokenized_sentence[index-1] in negation_word_list:\n",
    "                    negated = -1\n",
    "                else:\n",
    "                    negated = 1\n",
    "            # Find the index of the word then obtain the corresponding frequency and dfi\n",
    "            wIndex = list(np.where(df_frequency[\"word\"] == word.lower())[0])[0]\n",
    "            dfi = df_frequency[\"dfi\"][wIndex]\n",
    "            # Determine the frequency of this word in this particular headline\n",
    "            local_freq = sum(1 for _ in re.finditer(r'\\b%s\\b' % re.escape(word), ' '.join(sentence)))\n",
    "            # Calculate the weight assigned to this word               \n",
    "            weight = ((1+np.log(local_freq))/(1+np.log(a)))*np.log(N/dfi)\n",
    "            # Add this word score to the headline's score\n",
    "            if np.float(weight) != 0:\n",
    "                headline_score += weight.item() * negated\n",
    "            else:\n",
    "                headlines_score += 1 * negated\n",
    "    \n",
    "    # ****** NGRAM Search ******\n",
    "    for ngram in pos_ngram_list:\n",
    "        if ngram in tokenized_sentence:\n",
    "            headline_score += 1\n",
    "\n",
    "    return headline_score\n",
    "\n",
    "def countNegative_extra(sentence, neg_lex_list, total_headline_count, doc_average_word_count):\n",
    "    # Sentence is a sentence broken up into a list of words\n",
    "    # Add a row to the df_frequency dataframe that represents the weight assigned to that word\n",
    "    # Use the Loughran & McDonald formula to create the weight:\n",
    "    # Wij = ((1+log(tfij))/(1+log(a)))*log(N/dfi)\n",
    "    # Where N represents the total number of headlines in the sample\n",
    "    # dfi the number of headlines containing at least one occurrence of the ith word\n",
    "    # tfij the raw count of the ith word in the jth document\n",
    "    # a the average word count in the document\n",
    "    N = total_headline_count\n",
    "    headline_score = 0\n",
    "    a = doc_average_word_count.item()\n",
    "    negation_word_list = ['not']\n",
    "    negated = 1\n",
    "    stop_list = [word for word in stopwords.words('english') if word not in ['not']]\n",
    "    tokenized_sentence = [word for word in sentence if word not in stop_list]\n",
    "    # Word Search\n",
    "    for index, word in enumerate(tokenized_sentence):\n",
    "        if word.upper() in neg_lex_list:\n",
    "            #Determine if this word is preceded by a negation word\n",
    "            if index > 0:\n",
    "                if tokenized_sentence[index-1] in negation_word_list:\n",
    "                    negated = -1\n",
    "                else:\n",
    "                    negated = 1\n",
    "                    \n",
    "            wIndex = list(np.where(df_frequency[\"word\"] == word.lower())[0])[0]\n",
    "            dfi = df_frequency[\"dfi\"][wIndex]\n",
    "            # Determine the frequency of this word in this particular headline\n",
    "            local_freq = sum(1 for _ in re.finditer(r'\\b%s\\b' % re.escape(word), ' '.join(sentence)))\n",
    "            # Calculate the weight assigned to this word                \n",
    "            weight = ((1+np.log(local_freq))/(1+np.log(a)))*np.log(N/dfi)\n",
    "            # Add this word score to the headline's score\n",
    "            if np.float(weight) != 0:\n",
    "                headline_score += weight.item() * negated\n",
    "            else:\n",
    "                headlines_score += 1 * negated\n",
    "    \n",
    "    # ****** NGRAM Search ******\n",
    "    for ngram in neg_ngram_list:\n",
    "        if ngram in tokenized_sentence:\n",
    "            headline_score += 1\n",
    "\n",
    "    return headline_score\n",
    "\n",
    "\n",
    "# Determine the sentence of the sentence. \n",
    "def calculateSentiment_extra(clean_headline, positive_list, negative_list, awc):\n",
    "    lexicon_negative_extra = countNegative_extra(clean_headline, negative_list, len(df_headlines), awc)\n",
    "    lexicon_positive_extra = countPositive_extra(clean_headline, positive_list, len(df_headlines), awc)\n",
    "    return lexicon_positive_extra-lexicon_negative_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_lex_list = loadPositive()\n",
    "neg_lex_list = loadNegative()\n",
    "# Indicator lists\n",
    "aaii_sent_list = []\n",
    "lexicon_sent_list = []\n",
    "market_sent_list = []\n",
    "# A list of ngrams that have more weight in the analysis.\n",
    "# ngram_list = []\n",
    "ngram_match_list = []\n",
    "# Where the final aggregate sentiment is stored\n",
    "final_sent_list = []\n",
    "final_sent_list_word = []\n",
    "new_range  = [i * i for i in range(5) if i % 2 == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frequency = wordFrequency(df_headlines, \"title\")\n",
    "\n",
    "N = len(df_headlines)\n",
    "average_word_count = df_frequency[1:2].sum(axis=1)/N\n",
    "word_occurrence = [0 for x in range(df_frequency.count(axis='columns')[1])]\n",
    "headlines_list = df_headlines['title'].tolist()\n",
    "word_list = df_frequency[0:1].values.tolist()[0]\n",
    "\n",
    "for word_index, word in enumerate(word_list):\n",
    "    for headline in headlines_list:\n",
    "        #print(word)\n",
    "        hd = RegexpTokenizer(r'\\w+').tokenize(headline)\n",
    "        hd_upper = [word.upper() for word in hd]\n",
    "        #print(hd_upper)\n",
    "        if word.upper() in hd_upper:\n",
    "            #print(word.upper())\n",
    "            word_occurrence[word_index] += 1\n",
    "\n",
    "# Add the number of occurrences to the data frame\n",
    "df_frequency = df_frequency.append([word_occurrence], ignore_index=True)\n",
    "df_frequency = df_frequency.T\n",
    "df_frequency.columns = [\"word\", \"frequency\", \"dfi\"]\n",
    "\n",
    "# Calculate the weight\n",
    "word_weight = [0 for x in range(df_frequency.count(axis='columns')[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "lexicon_sent_list = []\n",
    "for index, row in df_headlines.iterrows():\n",
    "    # First clean and tokenize the text\n",
    "    # print(\"INDEX: \" + str(index))\n",
    "    word_list = clean_Text(row[\"title\"])\n",
    "    # If one of the lists does not contain a certain date, an error will be thrown upon trying to access that date\n",
    "    # Catch that error and skip that date\n",
    "    try:\n",
    "        # row_index = df_prices.loc[df_prices['date'] == df_headlines.iloc[index]['date']]\n",
    "        # LEXICON SENTIMENT OF THE TEXT\n",
    "        # Set the sentiment to 1, -1 or 0 (Pos, Neg, Neutral)\n",
    "        # Set a flag to identify which list threw the error and to react accordingly\n",
    "        flag = 0\n",
    "        s = calculateSentiment_extra(word_list, pos_lex_list, neg_lex_list, average_word_count)\n",
    "        lexicon_sent_list += [s]\n",
    "        \n",
    "    except ValueError:\n",
    "        # If the lexicon sentiment throws an error\n",
    "        if flag == 0:\n",
    "            market_sent_list += [0]\n",
    "            lexicon_sent_list += [0]\n",
    "        # If the market sentiment throws an error\n",
    "        else:\n",
    "            market_sent_list += [0]\n",
    "            lexicon_sent_list[index] = 0\n",
    "        # skipcount += 1\n",
    "        # print(\"Skip: \" + str(skipcount))\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_headlines_sent.iloc[0]['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_headlines_sent = df_headlines.copy()\n",
    "df_headlines_sent['sent'] = lexicon_sent_list\n",
    "df_headlines_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "\n",
    "sentiment_dict = {}\n",
    "for index, row in df_headlines_sent.iterrows():\n",
    "    # If the date already exists, then just add to the sentiment score.\n",
    "    if row['date'] not in sentiment_dict:\n",
    "        sentiment_dict[row['date']] = row['sent']\n",
    "    # If the date doesn't exist, then append it to the end of the dictionary.\n",
    "    else:\n",
    "        sentiment_dict[row['date']] += row['sent']\n",
    "\n",
    "df_final_ngram = pd.Series(sentiment_dict).to_frame()\n",
    "df_final_ngram = df_final_ngram.reset_index()\n",
    "df_final_ngram.columns = ['date', 'sent']\n",
    "df_final_ngram['date'] = pd.to_datetime(df_final_ngram.date)\n",
    "df_final_ngram.sort_values(by='date', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TESTING\n",
    "This section is for testing the results of the script. Delete this section when finished.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = 0\n",
    "for index, row in df_final.iterrows():\n",
    "    if row['sent'] == df_final_ngram.iloc[index]['sent']:\n",
    "        match += 1\n",
    "    else:\n",
    "        print('False')\n",
    "match/len(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comparison = pd.DataFrame()\n",
    "df_comparison = pd.concat([df_final])\n",
    "df_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comparison = df_comparison.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found = 0\n",
    "for index, sentence in enumerate(headlines_list):\n",
    "    if 'not' in sentence:\n",
    "        found += 1\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "text = tokenizer.tokenize(headlines_list[4])\n",
    "countPositive_extra(text, pos_lex_list, len(headlines_list), average_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "text = tokenizer.tokenize(headlines_list[4])\n",
    "for word in (text):\n",
    "    if word in neg_lex_list:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = calculateSentiment(text, pos_lex_list, neg_lex_list, average_word_count)\n",
    "b = calculateSentiment_extra(text, pos_lex_list, neg_lex_list, average_word_count)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_comparison.iterrows():\n",
    "    if row['sent'] < -10:\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comparison = pd.concat([df_final])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comparison = pd.concat([df_final_ngram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_ngram = df_final_ngram.rename(index=str, columns={\"date\": \"date_2\", \"sent\": \"sent_2\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comparison = pd.concat([df_final, df_final_ngram], axis=1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_ngram_list = ['missing forecasts', 'lowest since', 'shorts increase', 'trade war', 'extends losses', 'losing streak', 'government shutdown', 'retraces weakness', ]\n",
    "pos_ngram_list = ['winning streak', 'job creation', 'matches concensus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_list_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_sent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_headlines_sent = clean_dataframe(df_headlines_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_headlines_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
